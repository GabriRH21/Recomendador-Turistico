{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTa8kxhIJx1QwZ7KjnF/Le",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabriRH21/Recomendador-Turistico/blob/main/Clasificador.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenamiento para clasificador**"
      ],
      "metadata": {
        "id": "JL1UwXHHkWLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importacion del librerias:"
      ],
      "metadata": {
        "id": "5tlp3FbikYr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3WbN_IEkNdY",
        "outputId": "72881aca-4d98-4468-81c7-a54e7140b832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import openpyxl\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaración de metodos que usaremos:"
      ],
      "metadata": {
        "id": "5nD4QNglk3VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_description(inf):\n",
        "  if inf is None:\n",
        "    return \"\"\n",
        "  bs = BeautifulSoup(inf, 'html.parser')\n",
        "  aux = bs.find_all('p')\n",
        "  paragraphs = ''\n",
        "  for x in aux:\n",
        "    paragraphs += str(x)\n",
        "  descArray = re.split('<p>|</p>|<br/>|\\\\xa0', paragraphs)\n",
        "  desc = ''\n",
        "  for x in descArray:\n",
        "    desc  += x\n",
        "  return desc\n",
        "\n",
        "def processActivities(data):\n",
        "  dictionary = word_tokenize(str(data))\n",
        "  dictionary = [word for word in dictionary if word.isalpha()]\n",
        "  dictionary = [word.lower() for word in dictionary]\n",
        "  stopWords = set(stopwords.words('spanish'))\n",
        "  dictionary = [word for word in dictionary if word not in stopWords]\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "iA3e7eKsk1R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clase"
      ],
      "metadata": {
        "id": "Mx6pDy2CHd11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Categories:\n",
        "  def __init__(self, name, count):\n",
        "    self.name = name\n",
        "    self.count = count\n",
        "    self.words = {}\n",
        "    self.logarithm = {}\n",
        "    self.finalSum = 0\n",
        "\n",
        "  def get_name(self):\n",
        "    return self.name\n",
        "\n",
        "  def get_count(self):\n",
        "    return self.count\n",
        "\n",
        "  def get_words(self):\n",
        "    return self.words\n",
        "\n",
        "  def get_specific_word(self, word):\n",
        "    return self.words[word]\n",
        "\n",
        "  def get_logarithm(self, word):\n",
        "    return self.logarithm[word]\n",
        "\n",
        "  def get_finalSum(self):\n",
        "    return self.finalSum\n",
        "\n",
        "  def had_log(self, word):\n",
        "    return word in self.logarithm\n",
        "\n",
        "  def set_logarithm(self, word, result):\n",
        "    self.logarithm[word] = result\n",
        "\n",
        "  def had(self, word):\n",
        "    return word in self.words\n",
        "\n",
        "  def add_words(self, word):\n",
        "    if word in self.words:\n",
        "      self.words[word] += 1\n",
        "    else:\n",
        "      self.words[word] = 1\n",
        "\n",
        "  def add_finalSum(self, logarithm):\n",
        "    self.finalSum += logarithm\n",
        "\n",
        "  def reset_finalSum(self):\n",
        "    self.finalSum = 0\n",
        "\n",
        "  def increment(self):\n",
        "    self.count = self.count + 1"
      ],
      "metadata": {
        "id": "Mn474z_wHcxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recogida de datos\n"
      ],
      "metadata": {
        "id": "Pe9IleAFlC97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wb = openpyxl.load_workbook('activities.xlsx')\n",
        "sheet = wb.active\n",
        "data = []\n",
        "i= 0\n",
        "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
        "    info = row[9].split(' | ')\n",
        "    desc = row[10]\n",
        "    full_string = 'La actividad es ' + info[0]\n",
        "    full_string += ', situada en ' + info[1]\n",
        "    full_string += ', proporcionada por ' + info[2]\n",
        "    full_string += '. La descripción es: ' + get_description(desc)\n",
        "    data.append(full_string)\n",
        "\n",
        "wb = openpyxl.load_workbook('CATEGORIAS.xlsx')\n",
        "sheet = wb.active\n",
        "catego = []\n",
        "for row in sheet.iter_rows(min_row=1, max_col=1, values_only=True):\n",
        "  catego.append(row[0])\n"
      ],
      "metadata": {
        "id": "Mt80oTNhlFgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba"
      ],
      "metadata": {
        "id": "BWGoDC1AfJV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## @title\n",
        "#dataToWork = []\n",
        "#for i in range(len(catego)):\n",
        "#  dataToWork.append(data[i])\n",
        "#\n",
        "#catego_series = pd.Series(catego)\n",
        "#factCatego = catego_series.factorize()[0]\n",
        "#\n",
        "#df = pd.DataFrame({\"Labels\": factCatego, \"text\": dataToWork, \"assignment_group\": catego})\n",
        "#print(df.head())"
      ],
      "metadata": {
        "id": "khTGDHb2H3rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## @title\n",
        "#from spacy.lang.es.stop_words import STOP_WORDS as es_stop\n",
        "#from io import StringIO\n",
        "#import string\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.svm import LinearSVC\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#\n",
        "#final_stopwords_list = list(es_stop)\n",
        "#final_stopwords_list.append('\\r\\n')\n",
        "#final_stopwords_list.append(string.punctuation)\n",
        "#\n",
        "#tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words=final_stopwords_list,max_features=5000)\n",
        "#features = tfidf.fit_transform(df.text).toarray()\n",
        "#labels = df.Labels"
      ],
      "metadata": {
        "id": "ajiJ595zfcKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## @title\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.svm import LinearSVC\n",
        "#\n",
        "#model = LinearSVC()\n",
        "#\n",
        "#X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.2, random_state=0)\n",
        "#model.fit(X_train, y_train)\n",
        "#y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "NkZc9vdvfK_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#from sklearn import metrics\n",
        "#\n",
        "#unic_label_train = df.groupby(['assignment_group'])['assignment_group'].size()\n",
        "#unic_label_train = unic_label_train.index.get_level_values(0).tolist()\n",
        "#\n",
        "#print(metrics.classification_report(y_test, y_pred,\n",
        "#                                  target_names=unic_label_train))"
      ],
      "metadata": {
        "id": "m0aFNU2dngGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file_alias = \"<nombre de tu fichero>\"\n",
        "#path_to_data = f'tu/ruta/relativa/{file_alias}.csv'try:\n",
        "#  df = pd.read_csv(path_to_data, low_memory=False)\n",
        "#  except FileNotFoundError:\n",
        "#    raise FileNotFoundError(f\"File {file_alias} at path {path_to_data} doesn't exist.\")"
      ],
      "metadata": {
        "id": "usSOHiNCIxup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de texto y creación del diccionario\n"
      ],
      "metadata": {
        "id": "Nm2VkEK_zDIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dictionary = processActivities(data)\n",
        "dictionary = list(set(dictionary))\n",
        "dictionary.sort()\n",
        "with open('vocabulario.txt', 'w') as f:\n",
        "  f.write(\"Palabras totales: \" + str(len(dictionary)) + \"\\n\")\n",
        "  f.write(\"\\n\".join(dictionary))"
      ],
      "metadata": {
        "id": "VnkuunCnzIHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_leisureCat = Categories('OCIO', 0)\n",
        "_sportsCat = Categories('DEPORTES', 0)\n",
        "_toursCat = Categories('TOURS', 0)\n",
        "_culturalCat = Categories('CULTURAL Y GASTRONÓMICO', 0)\n",
        "_welfareCat = Categories('BIENESTAR', 0)\n",
        "AllCategories = [_leisureCat, _sportsCat, _toursCat, _culturalCat, _welfareCat]\n",
        "selectedCat = 0\n",
        "for i in range(101):\n",
        "  activity = processActivities(data[i])\n",
        "  if catego[i] == AllCategories[0].get_name():\n",
        "    #ocio\n",
        "    selectedCat = 0\n",
        "\n",
        "  elif catego[i] == AllCategories[1].get_name():\n",
        "    #deportes\n",
        "    selectedCat = 1\n",
        "\n",
        "  elif catego[i] == AllCategories[2].get_name():\n",
        "    #tours\n",
        "    selectedCat = 2\n",
        "\n",
        "  elif catego[i] == AllCategories[3].get_name():\n",
        "    #cultural\n",
        "    selectedCat = 3\n",
        "\n",
        "  elif catego[i] == AllCategories[4].get_name():\n",
        "    #bienestar\n",
        "    selectedCat = 4\n",
        "\n",
        "  #Incrementamos el numero de actividades de ocio que hay\n",
        "  AllCategories[selectedCat].increment()\n",
        "  #recorremos las palabras claves de la descripcion de la actividad\n",
        "  for j in range(len(activity)):\n",
        "    AllCategories[selectedCat].add_words(activity[j])\n",
        "\n",
        "  #else:\n",
        "    #nada"
      ],
      "metadata": {
        "id": "6rLgb6qmlvPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creacion de cada fichero\n"
      ],
      "metadata": {
        "id": "xduziNT1U1t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(AllCategories)):\n",
        "  name = AllCategories[i].get_name() + \".txt\"\n",
        "  with open(name, 'w') as n:\n",
        "    n.write('Número de documentos (actividades) del corpus: ' + str(AllCategories[i].get_count()) + '\\n')\n",
        "    n.write('Número de palabras del corpus: ' + str(len(AllCategories[i].get_words())) + '\\n')\n",
        "    for word in dictionary:\n",
        "      if AllCategories[i].had(word):\n",
        "        toLog = AllCategories[i].get_specific_word(word)\n",
        "      else:\n",
        "        toLog = 0\n",
        "      logarithm = math.log((toLog + 1) / (len(AllCategories[i].get_words()) + len(dictionary)))\n",
        "      AllCategories[i].set_logarithm(word, logarithm)\n",
        "      n.write('Palabra: ' +  str(word) + ' Frec: ' +  str(toLog) + ' LogProb: ' + str(logarithm) + '\\n')"
      ],
      "metadata": {
        "id": "N6sBHqqTU0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Clasificador**"
      ],
      "metadata": {
        "id": "7ofZCGJ81YLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lectura"
      ],
      "metadata": {
        "id": "lVgoRXFB1dvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wb = openpyxl.load_workbook('activities.xlsx')\n",
        "sheet = wb.active\n",
        "data = []\n",
        "i= 0\n",
        "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
        "    info = row[9].split(' | ')\n",
        "    desc = row[10]\n",
        "    full_string = 'La actividad es ' + info[0]\n",
        "    full_string += ', situada en ' + info[1]\n",
        "    full_string += ', proporcionada por ' + info[2]\n",
        "    full_string += '. La descripción es: ' + get_description(desc)\n",
        "    data.append(full_string)\n",
        "\n",
        "# porcentajes de preferencia\n",
        "# mejoras: distancias, cultura , recomendacio vectorial 10\n"
      ],
      "metadata": {
        "id": "C1iHIzS41cwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clasificacion"
      ],
      "metadata": {
        "id": "849gr9XbojV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = ''\n",
        "vector = ''\n",
        "AllSums = [0, 0, 0, 0, 0]\n",
        "\n",
        "for i in range(len(data)):\n",
        "  activity = processActivities(data[i])\n",
        "  fullDesc = data[i]\n",
        "  for cat in range(len(AllCategories)):\n",
        "    AllCategories[cat].reset_finalSum()\n",
        "    for j in range(len(activity)):\n",
        "      if AllCategories[cat].had_log(activity[j]):\n",
        "        AllCategories[cat].add_finalSum(AllCategories[cat].get_logarithm(activity[j]))\n",
        "    AllSums[cat] = AllCategories[cat].get_finalSum()\n",
        "\n",
        "  for j in range(10):\n",
        "    result += activity[j] + \" \"\n",
        "  result += '... \\n'\n",
        "  for j in range(len(AllCategories)):\n",
        "    result += AllCategories[j].get_name()\n",
        "    result += ': '\n",
        "    result += str(AllSums[j])\n",
        "    result += '\\n'\n",
        "  index = AllSums.index(np.max(AllSums))            # calcular porcentaje sengun peso logaritmico  con medias y varianza\n",
        "  result += AllCategories[index].get_name()\n",
        "  result += '\\n\\n'                                  # Medida de similitud (coseno)\n",
        "  vector += AllCategories[index].get_name()\n",
        "  vector += '\\n'\n",
        "\n",
        "with open('classificationInfo.txt', 'w') as n:\n",
        "  n.write(result)\n",
        "\n",
        "with open('classification.txt', 'w') as n:\n",
        "  n.write(vector)"
      ],
      "metadata": {
        "id": "QskCz85JonHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}